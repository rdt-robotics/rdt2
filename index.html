<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="RDT-2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RDT 2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation"> -->
  <!-- <div class="navbar-brand"> -->
  <!-- <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- </a> -->
  <!-- </div> -->
  <!-- <div class="navbar-menu"> -->
  <!-- <div class="navbar-start" style="flex-grow: 1; justify-content: center;"> -->
  <!-- <a class="navbar-item" href="https://github.com/thu-ml/"> -->
  <!-- <span class="icon"> -->
  <!-- <i class="fas fa-home"></i> -->
  <!-- </span> -->
  <!-- </a> -->

  <!-- </div> -->
  <!-- </nav> -->


  <!-- Hero Video Section - Full Width like Figure AI -->
  <section class="hero-video-section">
    <!-- Full Width Video Container -->
    <div class="hero-video-container">
      <div class="hero-video-placeholder">
        <video style="width:100%; height:auto; margin: 0%;" muted loading="lazy" autoplay loop playsinline>
          <source src="./static/videos/background_compressed.mp4" type="video/mp4">
        </video>
        <div style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; background: rgba(0, 0, 0, 0.1);"></div>
      </div>
      
      <!-- Overlay Content - Title, Date, and Links inside video -->
      <div class="hero-overlay-content">
        <div class="hero-content">
            <h1 class="hero-title">RDT 2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data</h1>
            
            <!-- Author and Date Information -->
            <div class="hero-meta">
              <div class="hero-author">
                <a href="#author" class="author-link">RDT Team</a>
              </div>
              <span class="hero-date">September 26, 2025</span>
            </div>
            
            <!-- Action Links -->
            <div class="hero-links">
              <span class="link-block">
                <a href="https://github.com/thu-ml/RDT2" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://huggingface.co/collections/robotics-diffusion-transformer/rdt-2-68ce9ddbf7dc520a231220d5" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> 
                    <img src="static/svg/hf.svg" alt="huggingface" width="24" height="24">
                  </span>
                  <span>Model</span>
                </a>
              </span>
              <span class="link-block" style="opacity: 0.6; pointer-events: none; cursor: default;">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false"
                      data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg"
                      viewBox="0 0 448 512" data-fa-i2svg="">
                      <path fill="currentColor"
                        d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z">
                      </path>
                    </svg>
                  </span>
                  <span>Data</span>
                </a>
              </span>
              <span class="link-block" style="opacity: 0.6; pointer-events: none; cursor: default;">
                <a href="#" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://x.com/EthanNg51931527/status/1846933749071859771" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> 
                    <i class="fa-brands fa-twitter"></i>                            
                  </span>
                  <span>Twitter</span>
                </a>
              </span>   
              <span class="link-block">
                <a href="discord.html" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> 
                    <i class="fa-brands fa-discord"></i>
                  </span>
                  <span>Discord</span>
                </a>
              </span>
              <span class="link-block">
                <a href="feishu.html" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> 
                    <svg t="1758621592506" class="icon" viewBox="0 0 1335 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4804" width="200" height="200"><path d="M616.34246923 637.06814707s83.08051941-41.33100723 179.38003464-141.42593244c20.30472345-21.08400898 101.26385431-114.00661917 215.7611347-143.8359461 25.61541119-6.6527916 107.70017706-34.31743588 241.39097699 5.7724876 4.12732837 1.24108457 0.92359808 6.76824114-1.86162726 10.33275202s-32.45580863 27.76566275-122.5499002 215.41478522c-7.2156091 15.253797-87.22227887 171.54388367-310.83399646 125.6526112-12.17994736-2.49660105-198.81888518-60.75542588-201.28662241-71.9107575zM363.78172951 90.125s-19.20795063 3.08828074-4.0263099 24.53307028c3.85313516 5.26739447 224.75178287 173.54982266 342.98674886 385.18362899 7.30219647 13.06025217 37.80979047-27.54919473 37.80979047-27.54919473s114.00661917-120.73156623 200.88254938-144.61523161c6.27757993-1.71731469 13.63750076-3.40576721 1.70288361-25.97619178-2.75636233-5.25296338-69.68834967-185.38342136-132.52187165-211.51835596z" p-id="4805" fill="#ffffff"></path><path d="M154.54350493 388.23066187s1.54413996-28.86243558 24.07127124-13.32001346c6.95584699 4.77673325 437.72769223 453.14023288 796.28572541 386.15052035 0 0 3.72325451 1.65959033-6.0899741 11.27078119-3.59337305 3.50678567-364.04189566 327.25672175-798.04633337 48.25799204 0 0-19.68418076-13.33444537-16.22068918-49.91758155z" p-id="4806" fill="#ffffff"></path></svg>
                  </span>
                  <span>Feishu</span>
                </a>
              </span>
              <span class="link-block">
                <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon"> 
                    <svg t="1758713357279" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4694" width="200" height="200"><path d="M156.09136 606.57001a457.596822 457.596822 0 0 1 221.680239-392.516385 50.844091 50.844091 0 1 1 50.844091 86.943396 355.90864 355.90864 0 0 0-138.804369 152.532274h16.77855a152.532274 152.532274 0 1 1-152.532274 152.532274z m406.752731 0a457.596822 457.596822 0 0 1 221.680239-392.007944 50.844091 50.844091 0 1 1 50.844091 86.943396 355.90864 355.90864 0 0 0-138.804369 152.532274h16.77855a152.532274 152.532274 0 1 1-152.532274 152.532274z" fill="#ffffff" p-id="4695"></path></svg>
                  </span>
                  <span>BibTeX</span>
                </a>
              </span>
            </div>
          </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    
  </section> -->



  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Introducing RDT 2</h2>
        <div class="column is-full">
          <p class="content has-text-justified mb-4">
            RDT 2, the sequel to RDT-1B [<a href="https://rdt-robotics.github.io/rdt-robotics/">1</a>], is <i>the first foundation model</i> that can achieve <b>zero-shot deployment</b> on <b>unseen embodiments</b> for <b>simple open-vocabulary tasks</b> like picking, placing, pressing, wiping, etc. This milestone was made possible by multifaceted efforts:
          </p>
          <p class="content has-text-justified mb-4">
            <ul>
              <li><b>Hardware redesign:</b> We redesigned the UMI [<a href="https://umi-gripper.github.io/">2</a>] hardware by applying higher-strength materials and more precise tracking methods, ensuring its reliability for large-scale data collection.</li>
              <li><b>Large and diverse data:</b> We collected 10,000+ hours of human manipulation videos in 100+ different indoor scenes, covering the majority of household tasks that a gripper can do.</li>
              <li><b>VLA pretraining:</b> Employing Residual VQ as an action tokenizer, we pretrained Qwen2.5-VL-7B-Instruct [<a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">3</a>] on our UMI dataset, enabling superior instruction-following capability.</li>
              <li><b>Diffusion distillation:</b> We trained the RDT model as an action expert with flow-matching loss and then distilled it into a one-step generator, realizing ultra-fast inference speed.</li>
            </ul>
          </p>
          <p class="content has-text-justified mb-4">
            Currently, we have open-sourced code and weights for RDT2-VQ and RDT2-FM. Other components, including data, code, and weights for other models, will be released shortly.
          </p>
          <!-- <div style="margin-top: 5%;"> -->
          <figure class="video-figure" style="margin-top: 5%;">
            <video style="width:100%; height:auto; margin: 0%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/demo_compressed.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Our introduction video of RDT 2.</figcaption>
          </figure>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Vision</h2>
        <div class="column is-full">
          <p class="content has-text-justified mb-4">
            <b>The path to embodied superintelligence requires a new paradigm.</b>
          </p>
          <p class="content has-text-justified mb-4">
            Teleoperation, even of the highest quality and zero embodiment gap, has significant drawbacks: it is expensive and non-portable. It is difficult to access diverse scenes and tasks for collecting data, which is necessary for training a universal model.
          </p>
          <p class="content has-text-justified mb-4">
            Our vision is to break free from these constraints. We imagine a future built on wearable systems that seamlessly capture the richness of human activity at a global scale. This approach won't just gather data; it will mirror the very fabric of how we interact with the physical world, providing the essential foundation for embodied superintelligence.
          </p>
          <p class="content has-text-justified mb-4">
            While the ultimate hardware for this vision is on its way, we take a foundational first step by scaling up embodiment-free human data with grippers.
          </p>
        </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">UMI Hardware</h2>
        <div class="column is-full">
          <p class="content has-text-justified mb-4">
            The original UMI [<a href="https://umi-gripper.github.io/">2</a>], manufactured using 3D printing, lacks the requisite strength for long-term, high-frequency data collection. To address this limitation, we redesigned the mechanical structure. The new product utilizes a robust nylon 66 and glass fiber composite material, fabricated using CNC precision machining. We abandoned the original SLAM tracking method since it frequently fails in texture-less indoor environments. Instead, we adopted an infrared light-based positioning system (HTC VIVE Tracker 3.0 [<a href="https://www.vive.com/us/accessory/tracker3/">4</a>]) to track the 6DoF pose of the end-effector.
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/umi.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Composition of our UMI hardware.</figcaption>
          </figure>
          <p class="content has-text-justified mb-4">
            Since our hardware provides a unified end-effector across robots and humans, the embodiment gap is minimized, and models trained using such UMI data can be <b>zero-shot</b> deployed on any robot arm. No tele-operation. No human data collection. No fine-tuning. It is totally plug-and-play. All you need to do is: purchase the specified camera and gripper, use the correct flange and 3D printed camera bracket for mounting, and align the TCP coordinate system.
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/deployment.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Easily deployable on any robot arm.</figcaption>
          </figure>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Dataset</h2>
        <div class="column is-full">
          <p class="content has-text-justified mb-4">
            We manufactured nearly <b>100</b> UMIs and distributed them to <b>100+</b> real-world home and office scenes for data collection. We collected <b>10,000+</b> hours of manipulation data, covering the vast majority of common human manipulation tasks*. Thanks to our hardware's portability and cheapness, we can collect the same amount of data at about <b>1/10 cost</b> and <b>5Ã— speed</b> of teleoperation**. Here, we visualize some example clips from the dataset:
          </p>
          
          <!-- Dataset Video Carousel -->
          <div class="dataset-carousel-container">
            <div class="dataset-carousel">
              <button class="carousel-btn carousel-btn-prev" id="prevBtn">
                <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <defs>
                    <linearGradient id="arrowGradientLeft" x1="0%" y1="0%" x2="0%" y2="100%">
                      <stop offset="0%" style="stop-color:#58D8FB;stop-opacity:1" />
                      <stop offset="100%" style="stop-color:#BE6AEF;stop-opacity:1" />
                    </linearGradient>
                  </defs>
                  <path d="M19 12H5M5 12L12 19M5 12L12 5" stroke="url(#arrowGradientLeft)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
              </button>
              <div class="carousel-track-container">
                <div class="carousel-track" id="carouselTrack">
                  <div class="carousel-slide active">
                    <video autoplay muted loop playsinline>
                      <source src="./static/videos/dataset/038_ep_20250615_150427_combined_6-38s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/038_ep_20250624_093057_combined_0-7s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/040_ep_20250628_114524_combined_0-7s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/044_ep_20250625_163518_combined_0-8s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/046_ep_20250705_131508_combined.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/058_ep_20250626_094940_combined.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/071_ep_20250604_155522_combined_0-12s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/071_ep_20250619_202150_combined_2-10s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/071_ep_20250623_114753_combined_1-6s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/084_ep_20250609_145231_combined_4-11s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/023_ep_20250621_143026_combined_2-20s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/033_ep_20250616_095612_combined.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/033_ep_20250619_094318_combined_0-11s.mp4" type="video/mp4">
                    </video>
                  </div>
                  <div class="carousel-slide">
                    <video muted loop playsinline>
                      <source src="./static/videos/dataset/036_ep_20250621_135355_combined_0-7s.mp4" type="video/mp4">
                    </video>
                  </div>
                </div>
              </div>
              <button class="carousel-btn carousel-btn-next" id="nextBtn">
                <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
                  <defs>
                    <linearGradient id="arrowGradientRight" x1="0%" y1="0%" x2="0%" y2="100%">
                      <stop offset="0%" style="stop-color:#58D8FB;stop-opacity:1" />
                      <stop offset="100%" style="stop-color:#BE6AEF;stop-opacity:1" />
                    </linearGradient>
                  </defs>
                  <path d="M5 12H19M19 12L12 5M19 12L12 19" stroke="url(#arrowGradientRight)" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"/>
                </svg>
              </button>
            </div>
            <!-- Caption Display Area -->
            <div class="video-caption-container">
              <span class="video-caption prompt" id="videoCaption">Loading caption...</span>
            </div>
            <div class="carousel-indicators">
              <div class="indicator active" data-index="0"></div>
              <div class="indicator" data-index="1"></div>
              <div class="indicator" data-index="2"></div>
              <div class="indicator" data-index="3"></div>
              <div class="indicator" data-index="4"></div>
              <div class="indicator" data-index="5"></div>
              <div class="indicator" data-index="6"></div>
              <div class="indicator" data-index="7"></div>
              <div class="indicator" data-index="8"></div>
              <div class="indicator" data-index="9"></div>
              <div class="indicator" data-index="10"></div>
              <div class="indicator" data-index="11"></div>
              <div class="indicator" data-index="12"></div>
              <div class="indicator" data-index="13"></div>
            </div>
          </div>
          <p class="content has-text-justified mb-4">
            Since our hardware provides a unified end-effector across robots and humans, the embodiment gap is minimized, and models trained using such UMI data can be <b>zero-shot</b> deployed on any robot arm. No tele-operation. No human data collection. No fine-tuning. It is totally plug-and-play. All you need to do is: purchase the specified camera and gripper, use the correct flange and 3D printed camera bracket for mounting, and align the TCP coordinate system.
          </p>
          <p class="content has-text-justified mb-4">
            *Due to hardware limitations, we excluded tasks involving water contact, heat contact, or requiring five-finger dexterity. We also removed tasks requiring large quantities of consumables, such as cooking. 
            <br>
            **Cost estimates include equipment cost and labor cost. Speed estimates include manipulation speed and speed of transfer between different locations.
          </p>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
        <div class="column is-full">
          <h2 class="title is-3">Training</h2>
          <p class="content mb-4">
            The training process can be divided into three stages.
          </p>
          <p class="content mb-4">
            <b>Stage 1</b>
          </p>
          <p class="content has-text-justified mb-4">
            In Stage 1, we trained Qwen2.5-VL-7B-Instruct [<a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">3</a>], a VLM model once pretrained on Internet-scale text and image data, on pure UMI data (i.e., our 10,000-hour UMI dataset). The model accepts two wrist-view fisheye images and a language instruction as input, and outputs discrete action tokens. The action tokens were discretized from continuous robot actions (6DoF end-effector pose and gripper width of both hands) by residual vector quantization (RVQ) [<a href="https://arxiv.org/abs/1711.00937">5</a>][<a href="https://arxiv.org/abs/2012.09841">6</a>][<a href="https://arxiv.org/abs/2203.01941">7</a>]. 
          </p>
          <p class="content has-text-justified mb-4">
            We took several measures to stabilize VQ training and improve codebook utilization, including factorized codes, cosine similarity, EMA updates, and codebook restart [<a href="https://github.com/karpathy/deep-vector-quantization">8</a>][<a href="https://arxiv.org/abs/2107.03312">9</a>][<a href="https://arxiv.org/abs/2110.04627">10</a>]. We also decoupled the discretization of rotation, translation, and gripper width as we found it helpful to avoid conflicts among multiple training objectives. As a result, we efficiently compress an action chunk of 0.8 seconds long (30 Hz) into a fixed-length 27 tokens. At the same level of precision, this length is <b>1/3</b> that of FAST [<a href="https://www.physicalintelligence.company/research/fast">11</a>] and <b>1/8</b> that of binning [<a href="https://arxiv.org/abs/2212.06817">12</a>][<a href="https://arxiv.org/abs/2307.15818">13</a>].
          </p>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-VQ</i>. It is slower than other RDT2 variants as it needs to generate 27 tokens autoregressively (i.e., 27 forward passes) to obtain an action chunk.
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/vla_pretrain.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Stage 1 (RDT2-VQ): pretrain VLM with discrete action tokens.</figcaption>
          </figure>
          <p class="content mb-4">
            <b>Stage 2</b>
          </p>
          <p class="content has-text-justified mb-4">
            In Stage 2, we replaced the RVQ with a 400M RDT model (an improved version of RDT-1B [<a href="https://rdt-robotics.github.io/rdt-robotics/">1</a>]) as an action expert, which attends to the Qwen backbone's KV during denoising, following the best practice in Ï€0 [<a href="https://www.physicalintelligence.company/blog/pi0">14</a>] and Ï€0.5 [<a href="https://www.physicalintelligence.company/blog/pi05">15</a>]. The model can generate continuous robot actions without discretization errors through five diffusion denoising steps. We copied the weights from the outcome of Stage 1 into the Qwen backbone, freezed it, and trained the RDT model with flow-matching loss.
          </p>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-FM</i>. We then mixed a tiny amount of real-robot data of UR and Franka with the original UMI data for post-training. We call this post-trained model <i>RDT2-FM-Post</i> to distinguish it from the original. These two models are much faster than the first since they only require one forward pass of Qwen and five forward passes of the 400M RDT model. 
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/diffusion_train.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Stage 2 (RDT2-FM): train RDT action expert with flow-matching loss.</figcaption>
          </figure>
          <p class="content mb-4">
            <b>Stage 3</b>
          </p>
          <p class="content has-text-justified mb-4">
            In Stage 3, we distilled RDT2-FM into a one-step diffusion policy without performance drop, where the Qwen backbone still stayed frozen. The model can map pure noise directly to robot actions through only a single diffusion step, similar to GAN [<a href="https://arxiv.org/abs/1406.2661">16</a>].
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/speed.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Comparisons of inference speed between ours and baselines.</figcaption>
          </figure>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-UltraFast</i>. The model is the fastest since it only requires one forward pass of Qwen and one forward pass of the 400M RDT model. This is crucial for many tasks that require real-time responses, such as playing table tennis.
          </p>
          <figure class="video-figure">
            <video autoplay loop muted playsinline>
                <source src="static/videos/distillation.mp4" type="video/mp4">
            </video>
            <figcaption class="video-caption">Stage 3 (RDT2-UltraFast): distill RDT2-FM into a one-step diffusion policy.</figcaption>
          </figure>
          <p class="content mb-4">
            <b>Model Family</b>
          </p>
          <p class="content has-text-justified mb-4">
            We list the model family of RDT2 as follows:
          </p>
          <p class="content has-text-justified mb-4">
            <ul style="list-style-type: disc; margin-left: 2%; margin-bottom: 2%;">
              <li><b>RDT2-VQ</b>: Stage 1, superior instruction following, slow inference, RL support, releasedðŸŽ‰</li>
              <li><b>RDT2-FM</b>: Stage 2, better performance, fast inference, no RL support for now, releasedðŸŽ‰</li>
              <li><b>RDT2-FM-Post</b>: Stage 2, twin of RDT2-FM, optimized performance on UR and Franka, comingðŸ”œ</li>
              <li><b>RDT2-UltraFast</b>: Stage 3, better performance, ultra-fast inference, no RL support for now, comingðŸ”œ</li>
            </ul>
          </p>
          <p class="content has-text-justified mb-4">
            More models and code, including reinforcement learning, are coming soon. Stay tuned!
          </p>
        </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Inference Video Samples</h2>
      <div class="column is-full">
          <p class="content mb-4">
            <b>Zero-Shot Deployment</b>
          </p>
          <p class="content has-text-justified mb-4">
            Setting: <b>4U</b>, Unseen Embodiment, Unseen Scene, Unseen Object, Unseen Language
            <br>
            Model: RDT2-VQ & RDT2-FM
            <br>
            Note: The instruction prompt, model used, and play speed are shown below each video accordingly.
          </p>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/1.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/1_5.mp4" type="video/mp4">
            </video>
          <p class="content has-text-justified mb-4">
            In the following, we tested policy robustness against various language instructions:
          </p>
        </div>

        <div class="column is-full">
          <p class="content mb-4">
            <b>Downstream Tasks</b>
          </p>
          <p class="content has-text-justified mb-4">
            Setting: The model is fine-tuned for each downstream task.
            <br>
            Model: RDT2-UltraFast
            <br>
            Note: The instruction prompt and play speed are shown below each video accordingly.
          </p>
          <div style="display: flex; justify-content: center; flex-wrap: wrap;">
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/2.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/3.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/4.mp4" type="video/mp4">
            </video>
            <img src="./static/videos/cups.png" style="width:45%; height:auto; margin: 2%;"
              alt="Water level pouring with left hand" style="max-width: 45%; margin: 5px;">
          </div>
        </div>

      <script>
        document.addEventListener('DOMContentLoaded', function () {
          const video1 = document.getElementById('video1');
          const video2 = document.getElementById('video2');
          let video1Ended = false;
          let video2Ended = false;

          function checkBothEnded() {
            if (video1Ended && video2Ended) {
              setTimeout(() => {
                video1.play();
                video2.play();
                video1Ended = false;
                video2Ended = false;
              }, 1000);
            }
          }

          video1.addEventListener('ended', function () {
            video1Ended = true;
            checkBothEnded();
          });

          video2.addEventListener('ended', function () {
            video2Ended = true;
            checkBothEnded();
          });

          video1.play();
          video2.play();
        });
      </script>
    </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
        <div class="column is-full">
          <h2 class="title is-3">Quantitative Comparison with Baselines</h2>
          <p class="content has-text-justified mb-4">
            ðŸš§We are still working on it. Stay tuned!
          </p>
        </div>
    </div>
  </section> -->

<section class="section" id="author">
    <div class="container is-max-desktop">
        <div class="column is-full">
          <h2 class="title is-3">Author Team</h2>
          <p class="content mb-4">
            <b>Core Team</b>
          </p>
          <p class="content has-text-justified mb-4">
            We are proud to note that all team members contributed equally to the success of this project.
            <ul style="list-style-type: disc; margin-left: 2%; margin-bottom: 2%;">
              <li>Songming Liu: Team Leader, Data Quality, Model, DEMO Screenwriter & Editor (part)</li>
              <li>Bangguo Li: Data Collection, Annotation, Deployment, DEMO Tech.</li>
              <li>Kai Ma: UMI Hardware, Data Collection, RL, Deployment</li>
              <li>Lingxuan Wu: Data Curation, Model, Training</li>
            </ul>
          </p>

          <p class="content mb-4">
            <b>Other Contributors</b>
          </p>
          <p class="content has-text-justified mb-4">
            Hengkai Tan, Xiao Ouyang, Zhengyi Wang, Huayu Chen
          </p>

          <p class="content mb-4">
            <b>Advisors</b>
          </p>
          <p class="content has-text-justified mb-4">
            Hang Su, Jun Zhu
          </p>
        </div>
    </div>
    <!--/ Abstract. -->
  </section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title is-3">Citation</h2>
        <p class="content has-text-justified mb-4">
            If you find our work helpful, please cite us:
        </p>
        <div class="bibtex-container">
            <pre><code id="bibtex-code">@software{rdt2,
    title={RDT2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data},
    author={RDT Team},
    url={https://github.com/thu-ml/RDT2},
    month={September},
    year={2025}
}</code></pre>
            <button onclick="copyToClipboard()" class="copy-button">
                <!-- ç²˜è´´å›¾æ ‡ï¼Œå¯ä»¥ä½¿ç”¨Font Awesome å›¾æ ‡åº“ -->
                <i class="fa fa-copy" aria-hidden="true"></i>
            </button>
        </div>
        <p class="content has-text-justified mb-4">
            Thank you!
        </p>
    </div>
</section>

<!-- å¼•å…¥Font Awesomeæ ·å¼è¡¨ -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<script>
function copyToClipboard() {
    var copyText = document.getElementById("bibtex-code").innerText;
    navigator.clipboard.writeText(copyText).then(function() {
        alert("Copied to clipboard!");
    }, function(err) {
        console.error('Unable to copy text:', err);
    });
}
</script>


  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <p>We borrowed the page template from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span></a>.</p>
      </div> -->
    </div>
  </footer>

  <!-- Dataset Video Carousel JavaScript -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      let currentIndex = 0;
      const carouselTrack = document.getElementById('carouselTrack');
      const slides = document.querySelectorAll('.carousel-slide');
      const prevBtn = document.getElementById('prevBtn');
      const nextBtn = document.getElementById('nextBtn');
      const indicators = document.querySelectorAll('.indicator');
      const totalSlides = slides.length;

      // Video filenames (without extension) for loading captions
      const videoNames = [
        '038_ep_20250615_150427_combined_6-38s',
        '038_ep_20250624_093057_combined_0-7s',
        '040_ep_20250628_114524_combined_0-7s',
        '044_ep_20250625_163518_combined_0-8s',
        '046_ep_20250705_131508_combined',
        '058_ep_20250626_094940_combined',
        '071_ep_20250604_155522_combined_0-12s',
        '071_ep_20250619_202150_combined_2-10s',
        '071_ep_20250623_114753_combined_1-6s',
        '084_ep_20250609_145231_combined_4-11s',
        '023_ep_20250621_143026_combined_2-20s',
        '033_ep_20250616_095612_combined',
        '033_ep_20250619_094318_combined_0-11s',
        '036_ep_20250621_135355_combined_0-7s'
      ];

      // Function to load caption for a video
      async function loadCaption(videoName) {
        try {
          const response = await fetch(`./static/videos/dataset/${videoName}.txt`);
          if (response.ok) {
            const caption = await response.text();
            return caption.trim();
          }
        } catch (error) {
          console.log('Could not load caption for:', videoName);
        }
        return videoName; // Fallback to filename if caption loading fails
      }

      // Load all captions
      const captions = {};
      async function loadAllCaptions() {
        for (let i = 0; i < videoNames.length; i++) {
          captions[i] = await loadCaption(videoNames[i]);
        }
        // Update the initial caption display
        updateCaptionDisplay(0);
      }

      function updateCaptionDisplay(index) {
        const captionElement = document.getElementById('videoCaption');
        if (captionElement && captions[index]) {
          captionElement.textContent = captions[index];
        }
      }

      function updateCarousel(index, animated = true) {
        currentIndex = index;
        
        // Calculate transform percentage
        const translateX = -(currentIndex * (100 / totalSlides));
        
        // Apply transition only if animated
        if (animated) {
          carouselTrack.style.transition = 'transform 0.6s cubic-bezier(0.25, 0.1, 0.25, 1)';
        } else {
          carouselTrack.style.transition = 'none';
        }
        
        // Move the carousel track
        carouselTrack.style.transform = `translateX(${translateX}%)`;
        
        // Update active slide
        slides.forEach((slide, i) => {
          const video = slide.querySelector('video');
          if (i === currentIndex) {
            slide.classList.add('active');
            // Play the current video
            video.play().catch(e => {
              console.log('Auto-play prevented:', e);
            });
          } else {
            slide.classList.remove('active');
            // Pause other videos
            video.pause();
          }
        });
        
        // Update indicators
        indicators.forEach((indicator, i) => {
          indicator.classList.toggle('active', i === currentIndex);
        });

        // Update caption display
        updateCaptionDisplay(currentIndex);
      }

      function nextSlide() {
        const newIndex = (currentIndex + 1) % totalSlides;
        updateCarousel(newIndex);
      }

      function prevSlide() {
        const newIndex = (currentIndex - 1 + totalSlides) % totalSlides;
        updateCarousel(newIndex);
      }

      // Event listeners for buttons
      nextBtn.addEventListener('click', nextSlide);
      prevBtn.addEventListener('click', prevSlide);

      // Event listeners for indicators
      indicators.forEach((indicator, index) => {
        indicator.addEventListener('click', () => {
          updateCarousel(index);
        });
      });

      // Keyboard navigation
      document.addEventListener('keydown', function(e) {
        // Only handle keyboard events when carousel is in view
        const carouselContainer = document.querySelector('.dataset-carousel-container');
        const rect = carouselContainer.getBoundingClientRect();
        const isInView = rect.top < window.innerHeight && rect.bottom > 0;
        
        if (isInView) {
          if (e.key === 'ArrowLeft') {
            e.preventDefault();
            prevSlide();
          } else if (e.key === 'ArrowRight') {
            e.preventDefault();
            nextSlide();
          }
        }
      });

      // Touch/swipe support for mobile
      let touchStartX = 0;
      let touchEndX = 0;
      let isDragging = false;

      carouselTrack.addEventListener('touchstart', function(e) {
        touchStartX = e.changedTouches[0].screenX;
        isDragging = true;
      }, { passive: true });

      carouselTrack.addEventListener('touchmove', function(e) {
        if (!isDragging) return;
        
        touchEndX = e.changedTouches[0].screenX;
        const diff = touchStartX - touchEndX;
        
        // Optional: Add real-time dragging visual feedback
        const dragDistance = diff / window.innerWidth * 100;
        const baseTranslateX = -(currentIndex * (100 / totalSlides));
        carouselTrack.style.transition = 'none';
        carouselTrack.style.transform = `translateX(${baseTranslateX - dragDistance}%)`;
      }, { passive: true });

      carouselTrack.addEventListener('touchend', function(e) {
        if (!isDragging) return;
        isDragging = false;
        
        const swipeThreshold = 50;
        const diff = touchStartX - touchEndX;
        
        if (Math.abs(diff) > swipeThreshold) {
          if (diff > 0) {
            // Swiped left - next slide
            nextSlide();
          } else {
            // Swiped right - previous slide
            prevSlide();
          }
        } else {
          // Snap back to current slide
          updateCarousel(currentIndex);
        }
      }, { passive: true });

      // Handle video ended events to loop individual videos
      slides.forEach((slide, index) => {
        const video = slide.querySelector('video');
        video.addEventListener('ended', function() {
          // Loop the current video if it's the active one
          if (index === currentIndex) {
            video.currentTime = 0;
            video.play().catch(e => {
              console.log('Auto-play prevented:', e);
            });
          }
        });
      });

      // Initialize carousel
      updateCarousel(0, false);
      
      // Load all captions
      loadAllCaptions();
      
      // Restore transition after initialization
      setTimeout(() => {
        carouselTrack.style.transition = 'transform 0.6s cubic-bezier(0.25, 0.1, 0.25, 1)';
      }, 100);
    });
  </script>

</body>

</html>

<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="RDT-2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>RDT 2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <!-- <link rel="stylesheet" href="./static/css/fontawesome.all.min.css"> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation"> -->
  <!-- <div class="navbar-brand"> -->
  <!-- <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false"> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- <span aria-hidden="true"></span> -->
  <!-- </a> -->
  <!-- </div> -->
  <!-- <div class="navbar-menu"> -->
  <!-- <div class="navbar-start" style="flex-grow: 1; justify-content: center;"> -->
  <!-- <a class="navbar-item" href="https://github.com/thu-ml/"> -->
  <!-- <span class="icon"> -->
  <!-- <i class="fas fa-home"></i> -->
  <!-- </span> -->
  <!-- </a> -->

  <!-- </div> -->
  <!-- </nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">RDT 2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data</h1>
            <div class="container">
              <div class="columns is-centered">
                <div class="column is-8">
                  <div class="is-size-5 publication-authors has-text-centered">
                    <span class="author-block">
                      <a href="#author">RDT Team</a>
                    </span>
                    <!-- <span class="author-block">
                      Songming Liu<sup>*,1</sup>,
                    </span>
                    <span class="author-block">
                      Lingxuan Wu<sup>*,1</sup>,
                    </span>
                    <span class="author-block">
                      Bangguo Li<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Hengkai Tan<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Huayu Chen<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Zhengyi Wang<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Ke Xu<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Hang Su<sup>1</sup>,
                    </span>
                    <span class="author-block">
                      Jun Zhu<sup>1</sup>
                    </span> -->
                  </div>
                </div>
              </div>
            </div>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Tsinghua University</span><br>
              <span class="author-block"><sup>*</sup>denotes equal contribution</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2410.07864" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/thu-ml/RDT2" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Model Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/collections/robotics-diffusion-transformer/rdt-2-68ce9ddbf7dc520a231220d5" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <img src="static/svg/hf.svg" alt="huggingface" width="24" height="24">
                    </span>
                    <span>Model</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block" style="opacity: 0.6; pointer-events: none; cursor: default;">
                  <a href="https://huggingface.co/datasets/robotics-diffusion-transformer/rdt-ft-data" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <svg class="svg-inline--fa fa-database fa-w-14" aria-hidden="true" focusable="false"
                        data-prefix="fas" data-icon="database" role="img" xmlns="http://www.w3.org/2000/svg"
                        viewBox="0 0 448 512" data-fa-i2svg="">
                        <path fill="currentColor"
                          d="M448 73.143v45.714C448 159.143 347.667 192 224 192S0 159.143 0 118.857V73.143C0 32.857 100.333 0 224 0s224 32.857 224 73.143zM448 176v102.857C448 319.143 347.667 352 224 352S0 319.143 0 278.857V176c48.125 33.143 136.208 48.572 224 48.572S399.874 209.143 448 176zm0 160v102.857C448 479.143 347.667 512 224 512S0 479.143 0 438.857V336c48.125 33.143 136.208 48.572 224 48.572S399.874 369.143 448 336z">
                        </path>
                      </svg>
                    </span>
                    <span>Data</span>
                  </a>
                </span>
                <!-- PDF Link. -->
                <span class="link-block" style="opacity: 0.6; pointer-events: none; cursor: default;">
                  <a href="#" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://x.com/EthanNg51931527/status/1846933749071859771" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <i class="fa-brands fa-twitter"></i>                            
                    </span>
                    <span>Twitter</span>
                  </a>
                </span>   
                <span class="link-block">
                  <a href="discord.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <i class="fa-brands fa-discord"></i>
                    </span>
                    <span>Discord</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://rdt-robotics.github.io/rdt-robotics/feishu.html" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <svg t="1758621592506" class="icon" viewBox="0 0 1335 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4804" width="200" height="200"><path d="M616.34246923 637.06814707s83.08051941-41.33100723 179.38003464-141.42593244c20.30472345-21.08400898 101.26385431-114.00661917 215.7611347-143.8359461 25.61541119-6.6527916 107.70017706-34.31743588 241.39097699 5.7724876 4.12732837 1.24108457 0.92359808 6.76824114-1.86162726 10.33275202s-32.45580863 27.76566275-122.5499002 215.41478522c-7.2156091 15.253797-87.22227887 171.54388367-310.83399646 125.6526112-12.17994736-2.49660105-198.81888518-60.75542588-201.28662241-71.9107575zM363.78172951 90.125s-19.20795063 3.08828074-4.0263099 24.53307028c3.85313516 5.26739447 224.75178287 173.54982266 342.98674886 385.18362899 7.30219647 13.06025217 37.80979047-27.54919473 37.80979047-27.54919473s114.00661917-120.73156623 200.88254938-144.61523161c6.27757993-1.71731469 13.63750076-3.40576721 1.70288361-25.97619178-2.75636233-5.25296338-69.68834967-185.38342136-132.52187165-211.51835596z" p-id="4805" fill="#ffffff"></path><path d="M154.54350493 388.23066187s1.54413996-28.86243558 24.07127124-13.32001346c6.95584699 4.77673325 437.72769223 453.14023288 796.28572541 386.15052035 0 0 3.72325451 1.65959033-6.0899741 11.27078119-3.59337305 3.50678567-364.04189566 327.25672175-798.04633337 48.25799204 0 0-19.68418076-13.33444537-16.22068918-49.91758155z" p-id="4806" fill="#ffffff"></path></svg>
                    </span>
                    <span>Feishu</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="#BibTeX" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"> 
                      <svg t="1758713357279" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4694" width="200" height="200"><path d="M156.09136 606.57001a457.596822 457.596822 0 0 1 221.680239-392.516385 50.844091 50.844091 0 1 1 50.844091 86.943396 355.90864 355.90864 0 0 0-138.804369 152.532274h16.77855a152.532274 152.532274 0 1 1-152.532274 152.532274z m406.752731 0a457.596822 457.596822 0 0 1 221.680239-392.007944 50.844091 50.844091 0 1 1 50.844091 86.943396 355.90864 355.90864 0 0 0-138.804369 152.532274h16.77855a152.532274 152.532274 0 1 1-152.532274 152.532274z" fill="#ffffff" p-id="4695"></path></svg>
                    </span>
                    <span>BibTeX</span>
                  </a>
                </span>           
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    
  </section> -->



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Introducing RDT 2</h2>
          <p class="content has-text-justified mb-4">
            RDT 2, the sequel to RDT-1B [<a href="https://rdt-robotics.github.io/rdt-robotics/">1</a>], is <i>the first foundation model</i> that can achieve <b>zero-shot deployment</b> on <b>unseen embodiments</b> for <b>simple open-vocabulary tasks</b> like picking, placing, pressing, wiping, etc. This milestone was made possible by multifaceted efforts:
            <ul style="list-style-type: disc; margin-left: 2%; margin-bottom: 2%;">
              <li><b>Hardware redesign:</b> We redesigned the UMI [<a href="https://umi-gripper.github.io/">2</a>] hardware by applying higher-strength materials and more precise tracking methods, ensuring its reliability for large-scale data collection.</li>
              <li><b>Large and diverse data:</b> We collected 10,000+ hours of human manipulation videos in 100+ different indoor scenes, covering the majority of household tasks that a gripper can do.</li>
              <li><b>VLA pretraining:</b> Employing Residual VQ as an action tokenizer, we pretrained Qwen2.5-VL-7B-Instruct [<a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">3</a>] on our UMI dataset, enabling superior instruction-following capability.</li>
              <li><b>Diffusion distillation:</b> We trained the RDT model as an action expert with flow-matching loss and then distilled it into a one-step generator, realizing ultra-fast inference speed.</li>
            </ul>
            Currently, we have open-sourced code and weights for RDT2-VQ and RDT2-FM. Other components, including data, code, and weights for other models, will be released shortly.
          </p>
          <div style="margin-top: 5%;">
            <video style="width:100%; height:auto; margin: 0%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/head.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Vision</h2>
          <p class="content has-text-justified mb-4">
            <b>The path to embodied superintelligence requires a new paradigm.</b>
          </p>
          <p class="content has-text-justified mb-4">
            Teleoperation, even of the highest quality and zero embodiment gap, has significant drawbacks: it is expensive and non-portable. It is difficult to access diverse scenes and tasks for collecting data, which is necessary for training a universal model.
          </p>
          <p class="content has-text-justified mb-4">
            Our vision is to break free from these constraints. We imagine a future built on wearable systems that seamlessly capture the richness of human activity at a global scale. This approach won't just gather data; it will mirror the very fabric of how we interact with the physical world, providing the essential foundation for embodied superintelligence.
          </p>
          <p class="content has-text-justified mb-4">
            While the ultimate hardware for this vision is on its way, we take a foundational first step by scaling up embodiment-free human data with grippers.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">UMI Hardware</h2>
          <p class="content has-text-justified mb-4">
            The original UMI [<a href="https://umi-gripper.github.io/">2</a>], manufactured using 3D printing, lacks the requisite strength for long-term, high-frequency data collection. To address this limitation, we redesigned the mechanical structure. The new product utilizes a robust nylon 66 and glass fiber composite material, fabricated using CNC precision machining. We abandoned the original SLAM tracking method since it frequently fails in texture-less indoor environments. Instead, we adopted an infrared light-based positioning system (HTC VIVE Tracker 3.0 [<a href="https://www.vive.com/us/accessory/tracker3/">4</a>]) to track the 6DoF pose of the end-effector.
          </p>
          <p class="content has-text-justified mb-4">
            Since our hardware provides a unified end-effector across robots and humans, the embodiment gap is minimized, and models trained using such UMI data can be <b>zero-shot</b> deployed on any robot arm. No tele-operation. No human data collection. No fine-tuning. It is totally plug-and-play. All you need to do is: purchase the specified camera and gripper, use the correct flange and 3D printed camera bracket for mounting, and align the TCP coordinate system.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Dataset</h2>
          <p class="content has-text-justified mb-4">
            We manufactured nearly <b>100</b> UMIs and distributed them to <b>100+</b> real-world home and office scenes for data collection. We collected <b>10,000+</b> hours of manipulation data, covering the vast majority of common human manipulation tasks*. Thanks to our hardware's portability and cheapness, we can collect the same amount of data at about <b>1/10 cost</b> and <b>5Ã— speed</b> of teleoperation**. Here, we visualize some example clips from the dataset:
          </p>
          <p class="content has-text-justified mb-4">
            Since our hardware provides a unified end-effector across robots and humans, the embodiment gap is minimized, and models trained using such UMI data can be <b>zero-shot</b> deployed on any robot arm. No tele-operation. No human data collection. No fine-tuning. It is totally plug-and-play. All you need to do is: purchase the specified camera and gripper, use the correct flange and 3D printed camera bracket for mounting, and align the TCP coordinate system.
          </p>
          <p class="content has-text-justified mb-4">
            *Due to hardware limitations, we excluded tasks involving water contact, heat contact, or requiring five-finger dexterity. We also removed tasks requiring large quantities of consumables, such as cooking. 
            <br>
            **Cost estimates include equipment cost and labor cost. Speed estimates include manipulation speed and speed of transfer between different locations.
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Training</h2>
          The training process can be divided into three stages.
          <h3 class="title is-4 has-text-centered">Stage 1</h3>
          <p class="content has-text-justified mb-4">
            In Stage 1, we trained Qwen2.5-VL-7B-Instruct [<a href="https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct">3</a>], a VLM model once pretrained on Internet-scale text and image data, on pure UMI data (i.e., our 10,000-hour UMI dataset). The model accepts two wrist-view fisheye images and a language instruction as input, and outputs discrete action tokens. The action tokens were discretized from continuous robot actions (6DoF end-effector pose and gripper width of both hands) by residual vector quantization (RVQ) [<a href="https://arxiv.org/abs/1711.00937">5</a>][<a href="https://arxiv.org/abs/2012.09841">6</a>][<a href="https://arxiv.org/abs/2203.01941">7</a>]. 
          </p>
          <p class="content has-text-justified mb-4">
            We took several measures to stabilize VQ training and improve codebook utilization, including factorized codes, cosine similarity, EMA updates, and codebook restart [<a href="https://github.com/karpathy/deep-vector-quantization">8</a>][<a href="https://arxiv.org/abs/2107.03312">9</a>][<a href="https://arxiv.org/abs/2110.04627">10</a>]. We also decoupled the discretization of rotation, translation, and gripper width as we found it helpful to avoid conflicts among multiple training objectives. As a result, we efficiently compress an action chunk of 0.8 seconds long (30 Hz) into a fixed-length 27 tokens. At the same level of precision, this length is <b>1/3</b> that of FAST [<a href="https://www.physicalintelligence.company/research/fast">11</a>] and <b>1/8</b> that of binning [<a href="https://arxiv.org/abs/2212.06817">12</a>][<a href="https://arxiv.org/abs/2307.15818">13</a>].
          </p>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-VQ</i>. It is slower than other RDT2 variants as it needs to generate 27 tokens autoregressively (i.e., 27 forward passes) to obtain an action chunk.
          </p>
          <h3 class="title is-4 has-text-centered">Stage 2</h3>
          <p class="content has-text-justified mb-4">
            In Stage 2, we replaced the RVQ with a 400M RDT model (an improved version of RDT-1B [<a href="https://rdt-robotics.github.io/rdt-robotics/">1</a>]) as an action expert, which attends to the Qwen backbone's KV during denoising, following the best practice in Ï€0 [<a href="https://www.physicalintelligence.company/blog/pi0">14</a>] and Ï€0.5 [<a href="https://www.physicalintelligence.company/blog/pi05">15</a>]. The model can generate continuous robot actions without discretization errors through five diffusion denoising steps. We copied the weights from the outcome of Stage 1 into the Qwen backbone, freezed it, and trained the RDT model with flow-matching loss.
          </p>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-FM</i>. We then mixed a tiny amount of real-robot data of UR and Franka with the original UMI data for post-training. We call this post-trained model <i>RDT2-FM-Post</i> to distinguish it from the original. These two models are much faster than the first since they only require one forward pass of Qwen and five forward passes of the 400M RDT model. 
          </p>
          <h3 class="title is-4 has-text-centered">Stage 3</h3>
          <p class="content has-text-justified mb-4">
            In Stage 3, we distilled RDT2-FM into a one-step diffusion policy without performance drop, where the Qwen backbone still stayed frozen. The model can map pure noise directly to robot actions through only a single diffusion step, similar to GAN [<a href="https://arxiv.org/abs/1406.2661">16</a>].
          </p>
          <p class="content has-text-justified mb-4">
            The outcome model in this stage is named <i>RDT2-UltraFast</i>. The model is the fastest since it only requires one forward pass of Qwen and one forward pass of the 400M RDT model. This is crucial for many tasks that require real-time responses, such as playing table tennis.
          </p>
          <h3 class="title is-4 has-text-centered">Model Family</h3>
          <p class="content has-text-justified mb-4">
            We list the model family of RDT2 as follows:
            <ul style="list-style-type: disc; margin-left: 2%; margin-bottom: 2%;">
              <li><b>RDT2-VQ</b>: Stage 1, superior instruction following, slow inference, RL support, releasedðŸŽ‰</li>
              <li><b>RDT2-FM</b>: Stage 2, better performance, fast inference, no RL support for now, releasedðŸŽ‰</li>
              <li><b>RDT2-FM-Post</b>: Stage 2, twin of RDT2-FM, optimized performance on UR and Franka, comingðŸ”œ</li>
              <li><b>RDT2-UltraFast</b>: Stage 3, better performance, ultra-fast inference, no RL support for now, comingðŸ”œ</li>
            </ul>
          </p>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Inference Video Samples</h2>
          <h3 class="title is-4 has-text-centered">Zero-Shot Deployment</h3>
          <p class="content has-text-justified mb-4">
            Setting: <b>4U</b>, Unseen Embodiment, Unseen Scene, Unseen Object, Unseen Language
            <br>
            Model: RDT2-VQ & RDT2-FM
            <br>
            Note: The instruction prompt, model used, and play speed are shown below each video accordingly.
          </p>
          <div class="columns is-centered">
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/1.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/1_5.mp4" type="video/mp4">
            </video>
          </div>
          <p class="content has-text-justified mb-4">
            In the following, we tested policy robustness against various language instructions:
          </p>
        </div>
      </div>

      <div class="columns is-centered">
        <div class="column content">
          <h3 class="title is-4" style="text-align: center;">Downstream Tasks</h3>
          <p class="content has-text-justified mb-4">
            Setting: The model is fine-tuned for each downstream task.
            <br>
            Model: RDT2-UltraFast
            <br>
            Note: The instruction prompt and play speed are shown below each video accordingly.
          </p>
          <div style="display: flex; justify-content: center; flex-wrap: wrap;">
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/2.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/3.mp4" type="video/mp4">
            </video>
            <video style="width:45%; height:auto; margin: 2%;" controls muted loading="lazy" autoplay loop playsinline>
              <source src="./static/videos/4.mp4" type="video/mp4">
            </video>
            <img src="./static/videos/cups.png" style="width:45%; height:auto; margin: 2%;"
              alt="Water level pouring with left hand" style="max-width: 45%; margin: 5px;">
          </div>
        </div>
      </div>

      <script>
        document.addEventListener('DOMContentLoaded', function () {
          const video1 = document.getElementById('video1');
          const video2 = document.getElementById('video2');
          let video1Ended = false;
          let video2Ended = false;

          function checkBothEnded() {
            if (video1Ended && video2Ended) {
              setTimeout(() => {
                video1.play();
                video2.play();
                video1Ended = false;
                video2Ended = false;
              }, 1000);
            }
          }

          video1.addEventListener('ended', function () {
            video1Ended = true;
            checkBothEnded();
          });

          video2.addEventListener('ended', function () {
            video2Ended = true;
            checkBothEnded();
          });

          video1.play();
          video2.play();
        });
      </script>
    </div>
    </div>
  </section>

  <!-- <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Quantitative Comparison with Baselines</h2>
          <p class="content has-text-justified mb-4">
            ðŸš§We are still working on it. Stay tuned!
          </p>
        </div>
      </div>
    </div>
  </section> -->

<section class="section" id="author">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-full">
          <h2 class="title is-3 has-text-centered">Author Team</h2>
          <h3 class="title is-4 has-text-centered">Core Team</h3>
          <p class="content has-text-justified mb-4">
            We are proud to note that all team members contributed equally to the success of this project.
            <ul style="list-style-type: disc; margin-left: 2%; margin-bottom: 2%;">
              <li>Songming Liu: Team Leader, Data Quality, Model, DEMO Screenwriter & Editor (part)</li>
              <li>Banguo Li: Data Collection, Annotation, Deployment, DEMO Tech.</li>
              <li>Kai Ma: UMI Hardware, Data Collection, RL, Deployment</li>
              <li>Lingxua Wu: Data Curation, Model, Training</li>
            </ul>
          </p>

          <h3 class="title is-4 has-text-centered">Other Contributors</h3>
          <p class="content has-text-justified mb-4">
            Hengkai Tan, Xiao Ouyang, Zhengyi Wang, Huayu Chen
          </p>

          <h3 class="title is-4 has-text-centered">Advisors</h3>
          <p class="content has-text-justified mb-4">
            Hang Su, Jun Zhu
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        If you find our work helpful, please cite us:
        <div style="position: relative;">
            <pre><code id="bibtex-code">@article{rdt2,
    title={RDT2: Enabling Zero-Shot Cross-Embodiment Generalization by Scaling Up UMI Data},
    author={RDT Team},
    url={https://github.com/thu-ml/RDT2},
    month={September},
    year={2025}
}</code></pre>
            <button onclick="copyToClipboard()" style="position: absolute; top: 5px; right: 5px; background: none; border: none; cursor: pointer;">
                <!-- ç²˜è´´å›¾æ ‡ï¼Œå¯ä»¥ä½¿ç”¨Font Awesome å›¾æ ‡åº“ -->
                <i class="fa fa-paste" aria-hidden="true"></i>
            </button>
        </div>
        Thank you!
    </div>
</section>

<!-- å¼•å…¥Font Awesomeæ ·å¼è¡¨ -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<script>
function copyToClipboard() {
    var copyText = document.getElementById("bibtex-code").innerText;
    navigator.clipboard.writeText(copyText).then(function() {
        alert("Copied to clipboard!");
    }, function(err) {
        console.error('Unable to copy text:', err);
    });
}
</script>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>We borrowed the page template from <a href="https://nerfies.github.io/"><span class="dnerf">Nerfies</span></a>.</p>
      </div>
    </div>
  </footer>

</body>

</html>
